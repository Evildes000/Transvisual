# ðŸ§  Transformer Visualization & Debugging Demo

A **minimal, self-contained Transformer implementation** with an **interactive web-based visualization tool**, designed for learning, debugging, and understanding how Transformers work **step by step**.

This project allows you to:
- Train a toy Transformer
- Inspect **encoder / decoder internals**
- Visualize **attention-related tensors**
- Observe **logits â†’ token â†’ text** generation in real time

---

## âœ¨ Features

- ðŸ”¹ Clean Transformer implementation (PyTorch, from scratch)
- ðŸ”¹ Encoder & Decoder layer-by-layer visualization
- ðŸ”¹ Self-attention & cross-attention internals (Q / K / V)
- ðŸ”¹ Mask visualization (padding mask & causal mask)
- ðŸ”¹ Final output logits inspection
- ðŸ”¹ Simple Flask-based interactive web UI

---

##  ðŸŽ¥ How to Train and Visualize the Model


This repository provides a **simple training pipeline** and an **interactive visualization interface** to help understand how a Transformer works internally.
- For visualization: python train.py web
- For training, just run train.py as usual



